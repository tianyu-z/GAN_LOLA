{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import torchvision.utils as vision_utils\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "from scipy import linalg\n",
    "import torchvision.datasets as _datasets\n",
    "import torchvision.transforms as _transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "from data import Binarize, Smooth, load_mnist, get_sampler\n",
    "from evals import compute_mu_sigma_pretrained_model, calculate_frechet_distance, _calculate_metrics, get_metrics\n",
    "from losses import get_disciminator_loss, get_generator_loss\n",
    "from model import DiscriminatorCNN28, GeneratorCNN28, MLP_mnist, pretrained_mnist_model\n",
    "from trainer import train,train_2nd_order_manual\n",
    "from updates import Lookahead, update_avg_gen, update_ema_gen\n",
    "from utils import save_models, get_plot_func, get_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NOISE_DIM = 8\n",
    "_H_FILTERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12672"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            eta = 1,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"both\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            eta = 5,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"lola\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            eta = 10,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"lola\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.0001,\n",
    "            lrG = 0.0001,\n",
    "            eta = 1,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"lola\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.0001,\n",
    "            lrG = 0.0001,\n",
    "            eta = 5,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"lola\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.0001,\n",
    "            lrG = 0.0001,\n",
    "            eta = 10,\n",
    "            eval_every = 5000,\n",
    "            n_workers = 4,\n",
    "            device = 'cuda',\n",
    "            type_ = \"lola\")\n",
    "\n",
    "\n",
    "for k in range(1,1+1):\n",
    "  exp_key = f\"iter{args['iterations']}_bs{args['batch_size']}_lrD{args['lrD']}\" + \\\n",
    "            f\"_lrG{args['lrG']}_eta{args['eta']}_type_{args['type_']}\" + f\"_ee{args['eval_every']}\"\n",
    "  out_dir = f\"/home/mila/t/tianyu.zhang/GAN_LOLA/drive/My Drive/results/final/{exp_key}/{k}/\"\n",
    "\n",
    "  shutil.rmtree(out_dir, ignore_errors=True)\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  with open(os.path.join(out_dir, 'args.json'), 'w') as fs:\n",
    "    json.dump(args, fs)\n",
    "\n",
    "  dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "\n",
    "  plot_func = get_plot_func(out_dir=out_dir, \n",
    "                            img_size=dataset[0][0].size(),\n",
    "                            num_samples_eval=10000)\n",
    "\n",
    "  G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "  D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)\n",
    "\n",
    "  train_2nd_order_manual(G, D, dataset, \n",
    "        iterations=args['iterations'], \n",
    "        batch_size=args['batch_size'], \n",
    "        lrD=args['lrD'], \n",
    "        lrG=args['lrG'], \n",
    "        eta=args['eta'], \n",
    "        eval_every=args['eval_every'], \n",
    "        n_workers=args['n_workers'], \n",
    "        device=torch.device(args['device']), \n",
    "        plot_func=plot_func,\n",
    "        out_dir=out_dir,\n",
    "        type_=args['type_'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
