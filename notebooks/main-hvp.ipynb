{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from time import time\n",
    "import copy\n",
    "import math\n",
    "import torchvision.utils as vision_utils\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "from scipy import linalg\n",
    "import torchvision.datasets as _datasets\n",
    "import torchvision.transforms as _transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from data import Binarize, Smooth, load_mnist, get_sampler\n",
    "from evals import compute_mu_sigma_pretrained_model, calculate_frechet_distance, _calculate_metrics, get_metrics\n",
    "from losses import get_disciminator_loss, get_generator_loss\n",
    "from model import DiscriminatorCNN28, GeneratorCNN28, MLP_mnist, pretrained_mnist_model\n",
    "from trainer import train\n",
    "from updates import Lookahead, update_avg_gen, update_ema_gen\n",
    "from utils import save_models, get_plot_func, get_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NOISE_DIM = 8\n",
    "_H_FILTERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GeneratorCNN28(noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=True)\n",
    "D = DiscriminatorCNN28(h_filters=_H_FILTERS, spectral_norm=False, img_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(iterations = 100000,\n",
    "            batch_size = 128,\n",
    "            lrD = 0.001,\n",
    "            lrG = 0.001,\n",
    "            beta1 = 0.05,\n",
    "            extragrad = False,\n",
    "            eval_every = 5000,\n",
    "            lookahead = False,\n",
    "            eval_avg = False,\n",
    "            lookahead_k = 1000,\n",
    "            n_workers = 5,\n",
    "            device = 'cuda',\n",
    "            grad_max_norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_tuple(Tuple):\n",
    "    return (x.detach_() for x in Tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0\n",
      "lossD:  1.4437634944915771\n",
      "lossG:  0.6276097297668457\n",
      "time cost:  0.18486309051513672\n",
      "iter:  1000\n",
      "lossD:  1.1064461469650269\n",
      "lossG:  0.6296601295471191\n",
      "time cost:  42.27976322174072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c22fa3c040a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Calculating the SGD terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mgradsD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mgradsG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    202\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# correct lola\n",
    "device = args[\"device\"]\n",
    "iterations = args[\"iterations\"]\n",
    "batch_size = args[\"batch_size\"]\n",
    "dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "n_workers = 4\n",
    "eta_lookahead = 1\n",
    "lr = 0.0001\n",
    "sampler = get_sampler(\n",
    "    dataset, batch_size, shuffle=True, num_workers=n_workers, drop_last=True\n",
    ")\n",
    "lbl_real = torch.ones(batch_size, 1, device=device)\n",
    "lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "for i in range(iterations):\n",
    "    if i ==0:\n",
    "        start = time()\n",
    "    x_real, _ = sampler()\n",
    "    x_real = x_real.to(device)\n",
    "\n",
    "    # loss D\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    x_gen = G(z)\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "\n",
    "    # loss G\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    lossG = get_generator_loss(G, D, z, lbl_real)\n",
    "\n",
    "    # Calculating the SGD terms\n",
    "    gradsD = torch.autograd.grad(lossD, D.parameters(), create_graph=True)\n",
    "    gradsG = torch.autograd.grad(lossG, G.parameters(), create_graph=True)\n",
    "\n",
    "    # Calculating the JVP\n",
    "    dLD_dG = torch.autograd.grad(lossD, G.parameters(), create_graph=True)\n",
    "    dLG_dD = torch.autograd.grad(lossG, D.parameters(), create_graph=True)\n",
    "    J_D_P_dLG_dD = torch.autograd.grad(gradsD, G.parameters(), grad_outputs = dLG_dD)\n",
    "    J_G_P_dLD_dG = torch.autograd.grad(gradsG, D.parameters(), grad_outputs = dLD_dG)\n",
    "\n",
    "    # Calculating the LookAhead Step\n",
    "    gradsD_LookAhead = detach_tuple(gradsD+J_G_P_dLD_dG * eta_lookahead)\n",
    "    gradsG_LookAhead = detach_tuple(gradsG+J_D_P_dLG_dD * eta_lookahead)\n",
    "\n",
    "    # Updating the Networks\n",
    "    for param, grad in zip(D.parameters(),gradsD_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    for param, grad in zip(G.parameters(),gradsG_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    if i%1000 == 0:\n",
    "        end = time()\n",
    "        print(\"iter: \", i)\n",
    "        print(\"lossD: \", lossD.item())\n",
    "        print(\"lossG: \", lossG.item())\n",
    "        print(\"time cost: \", end-start)\n",
    "        start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0\n",
      "lossD:  1.0804047584533691\n",
      "lossG:  0.67572021484375\n",
      "time cost:  0.06939530372619629\n",
      "iter:  1000\n",
      "lossD:  1.283009648323059\n",
      "lossG:  0.62162184715271\n",
      "time cost:  41.301108598709106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-90da6fba1cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Calculating the JVP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdLD_dG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mdLG_dD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mJ_D_P_dLG_dD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradsD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdLG_dD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    202\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wrong lola\n",
    "device = args[\"device\"]\n",
    "iterations = args[\"iterations\"]\n",
    "batch_size = args[\"batch_size\"]\n",
    "dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "n_workers = 4\n",
    "eta_lookahead = 1\n",
    "lr = 0.0001\n",
    "sampler = get_sampler(\n",
    "    dataset, batch_size, shuffle=True, num_workers=n_workers, drop_last=True\n",
    ")\n",
    "lbl_real = torch.ones(batch_size, 1, device=device)\n",
    "lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "for i in range(iterations):\n",
    "    if i ==0:\n",
    "        start = time()\n",
    "    x_real, _ = sampler()\n",
    "    x_real = x_real.to(device)\n",
    "\n",
    "    # loss D\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    x_gen = G(z)\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "\n",
    "    # loss G\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    lossG = get_generator_loss(G, D, z, lbl_real)\n",
    "\n",
    "    # Calculating the SGD terms\n",
    "    gradsD = torch.autograd.grad(lossD, D.parameters(), create_graph=True)\n",
    "    gradsG = torch.autograd.grad(lossG, G.parameters(), create_graph=True)\n",
    "\n",
    "    # Calculating the JVP\n",
    "    dLD_dG = torch.autograd.grad(lossD, G.parameters(), create_graph=True)\n",
    "    dLG_dD = torch.autograd.grad(lossG, D.parameters(), create_graph=True)\n",
    "    J_D_P_dLG_dD = torch.autograd.grad(gradsD, G.parameters(), grad_outputs = dLG_dD)\n",
    "    J_G_P_grad_D = torch.autograd.grad(gradsG, D.parameters(), grad_outputs = dLD_dG)\n",
    "\n",
    "    # Calculating the LookAhead Step\n",
    "    gradsD_LookAhead = detach_tuple(gradsD+J_D_P_dLG_dD * eta_lookahead)\n",
    "    gradsG_LookAhead = detach_tuple(gradsG+J_G_P_grad_D * eta_lookahead)\n",
    "\n",
    "    # Updating the Networks\n",
    "    for param, grad in zip(D.parameters(),gradsD_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    for param, grad in zip(G.parameters(),gradsG_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    if i%1000 == 0:\n",
    "        end = time()\n",
    "        print(\"iter: \", i)\n",
    "        print(\"lossD: \", lossD.item())\n",
    "        print(\"lossG: \", lossG.item())\n",
    "        print(\"time cost: \", end-start)\n",
    "        start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradsG[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_D_P_dLG_dD[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0\n",
      "lossD:  1.3404502868652344\n",
      "lossG:  0.7881301641464233\n",
      "time cost:  0.045519113540649414\n",
      "iter:  1000\n",
      "lossD:  1.2351443767547607\n",
      "lossG:  0.6493948101997375\n",
      "time cost:  25.780089139938354\n",
      "iter:  2000\n",
      "lossD:  1.1581275463104248\n",
      "lossG:  0.6537896394729614\n",
      "time cost:  27.07682728767395\n",
      "iter:  3000\n",
      "lossD:  1.0838878154754639\n",
      "lossG:  0.7969905138015747\n",
      "time cost:  25.37028741836548\n",
      "iter:  4000\n",
      "lossD:  1.1158428192138672\n",
      "lossG:  0.7977834343910217\n",
      "time cost:  27.26705527305603\n",
      "iter:  5000\n",
      "lossD:  1.2772674560546875\n",
      "lossG:  0.7501564025878906\n",
      "time cost:  27.002684116363525\n",
      "iter:  6000\n",
      "lossD:  1.0976250171661377\n",
      "lossG:  0.8970158696174622\n",
      "time cost:  26.98004961013794\n",
      "iter:  7000\n",
      "lossD:  0.973142147064209\n",
      "lossG:  1.0238088369369507\n",
      "time cost:  25.998576164245605\n",
      "iter:  8000\n",
      "lossD:  1.08255934715271\n",
      "lossG:  0.8506584167480469\n",
      "time cost:  26.949941873550415\n",
      "iter:  9000\n",
      "lossD:  0.9393980503082275\n",
      "lossG:  0.9697446823120117\n",
      "time cost:  26.440016269683838\n",
      "iter:  10000\n",
      "lossD:  0.9776016473770142\n",
      "lossG:  1.0517079830169678\n",
      "time cost:  27.682188034057617\n",
      "iter:  11000\n",
      "lossD:  0.9194340109825134\n",
      "lossG:  1.138771653175354\n",
      "time cost:  27.397358655929565\n",
      "iter:  12000\n",
      "lossD:  0.8279234170913696\n",
      "lossG:  1.1985492706298828\n",
      "time cost:  25.931380033493042\n",
      "iter:  13000\n",
      "lossD:  0.734083890914917\n",
      "lossG:  1.2903534173965454\n",
      "time cost:  26.583099126815796\n",
      "iter:  14000\n",
      "lossD:  0.7251925468444824\n",
      "lossG:  1.235558032989502\n",
      "time cost:  27.36527395248413\n",
      "iter:  15000\n",
      "lossD:  0.7283616065979004\n",
      "lossG:  1.3349002599716187\n",
      "time cost:  26.805166959762573\n",
      "iter:  16000\n",
      "lossD:  0.682855486869812\n",
      "lossG:  1.3439757823944092\n",
      "time cost:  25.62249994277954\n",
      "iter:  17000\n",
      "lossD:  0.7577199935913086\n",
      "lossG:  1.2964869737625122\n",
      "time cost:  26.739630222320557\n",
      "iter:  18000\n",
      "lossD:  0.7675360441207886\n",
      "lossG:  1.271265983581543\n",
      "time cost:  27.56326460838318\n",
      "iter:  19000\n",
      "lossD:  0.7684547901153564\n",
      "lossG:  1.1810638904571533\n",
      "time cost:  26.67918109893799\n",
      "iter:  20000\n",
      "lossD:  0.7922908067703247\n",
      "lossG:  1.2180402278900146\n",
      "time cost:  25.91043257713318\n",
      "iter:  21000\n",
      "lossD:  0.6726597547531128\n",
      "lossG:  1.3781050443649292\n",
      "time cost:  26.251214742660522\n",
      "iter:  22000\n",
      "lossD:  0.7433148622512817\n",
      "lossG:  1.101702332496643\n",
      "time cost:  27.168654203414917\n",
      "iter:  23000\n",
      "lossD:  0.7339602708816528\n",
      "lossG:  1.5872234106063843\n",
      "time cost:  26.325308799743652\n",
      "iter:  24000\n",
      "lossD:  0.8398247957229614\n",
      "lossG:  1.1606547832489014\n",
      "time cost:  26.562750577926636\n",
      "iter:  25000\n",
      "lossD:  0.7934785485267639\n",
      "lossG:  1.2415013313293457\n",
      "time cost:  27.110358953475952\n",
      "iter:  26000\n",
      "lossD:  0.7858820557594299\n",
      "lossG:  1.330941915512085\n",
      "time cost:  25.50351572036743\n",
      "iter:  27000\n",
      "lossD:  0.8188755512237549\n",
      "lossG:  1.4193110466003418\n",
      "time cost:  26.36186146736145\n",
      "iter:  28000\n",
      "lossD:  0.6860308051109314\n",
      "lossG:  1.1122981309890747\n",
      "time cost:  26.890440464019775\n",
      "iter:  29000\n",
      "lossD:  0.7218143939971924\n",
      "lossG:  1.5158259868621826\n",
      "time cost:  27.75073528289795\n",
      "iter:  30000\n",
      "lossD:  0.6716082692146301\n",
      "lossG:  1.179348349571228\n",
      "time cost:  27.612876653671265\n",
      "iter:  31000\n",
      "lossD:  0.7183053493499756\n",
      "lossG:  1.3389229774475098\n",
      "time cost:  26.88151526451111\n",
      "iter:  32000\n",
      "lossD:  0.7952060699462891\n",
      "lossG:  1.1395200490951538\n",
      "time cost:  27.39987540245056\n",
      "iter:  33000\n",
      "lossD:  0.7696208953857422\n",
      "lossG:  1.15359365940094\n",
      "time cost:  26.715186834335327\n",
      "iter:  34000\n",
      "lossD:  0.7564626336097717\n",
      "lossG:  1.391878604888916\n",
      "time cost:  27.176944732666016\n",
      "iter:  35000\n",
      "lossD:  0.6948418617248535\n",
      "lossG:  1.3254201412200928\n",
      "time cost:  25.824002981185913\n",
      "iter:  36000\n",
      "lossD:  0.6785752773284912\n",
      "lossG:  1.18434476852417\n",
      "time cost:  26.967742443084717\n",
      "iter:  37000\n",
      "lossD:  0.6741863489151001\n",
      "lossG:  1.3524110317230225\n",
      "time cost:  27.656143188476562\n",
      "iter:  38000\n",
      "lossD:  0.7278702259063721\n",
      "lossG:  1.4383869171142578\n",
      "time cost:  27.241652011871338\n",
      "iter:  39000\n",
      "lossD:  0.7552958726882935\n",
      "lossG:  1.2730118036270142\n",
      "time cost:  26.066402673721313\n",
      "iter:  40000\n",
      "lossD:  0.8077303171157837\n",
      "lossG:  1.1457327604293823\n",
      "time cost:  27.254371166229248\n",
      "iter:  41000\n",
      "lossD:  0.9746942520141602\n",
      "lossG:  0.7647217512130737\n",
      "time cost:  26.175581455230713\n",
      "iter:  42000\n",
      "lossD:  0.8570199012756348\n",
      "lossG:  1.1566619873046875\n",
      "time cost:  26.020638704299927\n",
      "iter:  43000\n",
      "lossD:  0.9255701899528503\n",
      "lossG:  1.039036750793457\n",
      "time cost:  26.67314052581787\n",
      "iter:  44000\n",
      "lossD:  0.9106540679931641\n",
      "lossG:  0.9576780796051025\n",
      "time cost:  27.072589635849\n",
      "iter:  45000\n",
      "lossD:  0.9201165437698364\n",
      "lossG:  1.0491458177566528\n",
      "time cost:  27.01738715171814\n",
      "iter:  46000\n",
      "lossD:  0.7648075222969055\n",
      "lossG:  1.136891484260559\n",
      "time cost:  26.19536328315735\n",
      "iter:  47000\n",
      "lossD:  0.8352873921394348\n",
      "lossG:  1.1008861064910889\n",
      "time cost:  26.810667753219604\n",
      "iter:  48000\n",
      "lossD:  0.7617993354797363\n",
      "lossG:  1.3439712524414062\n",
      "time cost:  26.824233293533325\n",
      "iter:  49000\n",
      "lossD:  0.9672568440437317\n",
      "lossG:  1.0848076343536377\n",
      "time cost:  27.643975734710693\n",
      "iter:  50000\n",
      "lossD:  0.9209902882575989\n",
      "lossG:  0.8740397691726685\n",
      "time cost:  26.37278652191162\n",
      "iter:  51000\n",
      "lossD:  0.8532329201698303\n",
      "lossG:  1.2727041244506836\n",
      "time cost:  26.238794803619385\n",
      "iter:  52000\n",
      "lossD:  0.8863306045532227\n",
      "lossG:  0.8303970098495483\n",
      "time cost:  27.136887550354004\n",
      "iter:  53000\n",
      "lossD:  0.8116965293884277\n",
      "lossG:  1.4020638465881348\n",
      "time cost:  27.07869267463684\n",
      "iter:  54000\n",
      "lossD:  0.7180401086807251\n",
      "lossG:  1.30631422996521\n",
      "time cost:  26.891230821609497\n",
      "iter:  55000\n",
      "lossD:  0.7972683906555176\n",
      "lossG:  1.132507562637329\n",
      "time cost:  26.571041345596313\n",
      "iter:  56000\n",
      "lossD:  1.1943256855010986\n",
      "lossG:  0.49385514855384827\n",
      "time cost:  26.091729879379272\n",
      "iter:  57000\n",
      "lossD:  0.6704609990119934\n",
      "lossG:  1.3956947326660156\n",
      "time cost:  26.755207300186157\n",
      "iter:  58000\n",
      "lossD:  0.9245768785476685\n",
      "lossG:  0.744371235370636\n",
      "time cost:  26.584420919418335\n",
      "iter:  59000\n",
      "lossD:  0.8949825167655945\n",
      "lossG:  1.0189168453216553\n",
      "time cost:  26.793211936950684\n",
      "iter:  60000\n",
      "lossD:  0.7798628211021423\n",
      "lossG:  1.6432007551193237\n",
      "time cost:  27.835007905960083\n",
      "iter:  61000\n",
      "lossD:  0.696336567401886\n",
      "lossG:  1.7791895866394043\n",
      "time cost:  27.296210527420044\n",
      "iter:  62000\n",
      "lossD:  0.5478709936141968\n",
      "lossG:  1.6691228151321411\n",
      "time cost:  27.104073524475098\n",
      "iter:  63000\n",
      "lossD:  0.6768038272857666\n",
      "lossG:  1.1690136194229126\n",
      "time cost:  26.048916578292847\n",
      "iter:  64000\n",
      "lossD:  0.4771820902824402\n",
      "lossG:  1.6787443161010742\n",
      "time cost:  26.237929821014404\n",
      "iter:  65000\n",
      "lossD:  0.2032221257686615\n",
      "lossG:  2.2671303749084473\n",
      "time cost:  26.951443910598755\n",
      "iter:  66000\n",
      "lossD:  0.17251572012901306\n",
      "lossG:  2.2750515937805176\n",
      "time cost:  26.72703266143799\n",
      "iter:  67000\n",
      "lossD:  0.16335268318653107\n",
      "lossG:  2.3081328868865967\n",
      "time cost:  27.66122603416443\n",
      "iter:  68000\n",
      "lossD:  0.11649156361818314\n",
      "lossG:  2.6824121475219727\n",
      "time cost:  27.207170486450195\n",
      "iter:  69000\n",
      "lossD:  0.1881997138261795\n",
      "lossG:  2.3237953186035156\n",
      "time cost:  26.112175703048706\n",
      "iter:  70000\n",
      "lossD:  0.1230035126209259\n",
      "lossG:  3.050579786300659\n",
      "time cost:  27.099270343780518\n",
      "iter:  71000\n",
      "lossD:  0.14557722210884094\n",
      "lossG:  2.171823740005493\n",
      "time cost:  27.04466938972473\n",
      "iter:  72000\n",
      "lossD:  0.19171519577503204\n",
      "lossG:  2.7068727016448975\n",
      "time cost:  25.927051305770874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2627b2ee7407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdLD_dG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdLG_dD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mJ_D_P_grad_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLD_dG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradsG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mJ_G_P_grad_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLG_dD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradsD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    202\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = args[\"device\"]\n",
    "iterations = args[\"iterations\"]\n",
    "batch_size = args[\"batch_size\"]\n",
    "dataset = load_mnist(_data_root='datasets', binarized=False)\n",
    "n_workers = 4\n",
    "eta_lookahead = 1\n",
    "lr = 0.0001\n",
    "sampler = get_sampler(\n",
    "    dataset, batch_size, shuffle=True, num_workers=n_workers, drop_last=True\n",
    ")\n",
    "lbl_real = torch.ones(batch_size, 1, device=device)\n",
    "lbl_fake = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "for i in range(iterations):\n",
    "    if i ==0:\n",
    "        start = time()\n",
    "    x_real, _ = sampler()\n",
    "    x_real = x_real.to(device)\n",
    "\n",
    "    # loss D\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    x_gen = G(z)\n",
    "    lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "\n",
    "    # loss G\n",
    "    z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "    lossG = get_generator_loss(G, D, z, lbl_real)\n",
    "\n",
    "    # Calculating the SGD terms\n",
    "    gradsD = torch.autograd.grad(lossD, D.parameters(), create_graph=True)\n",
    "    gradsG = torch.autograd.grad(lossG, G.parameters(), create_graph=True)\n",
    "\n",
    "    # Calculating the JVP\n",
    "    dLD_dG = torch.autograd.grad(lossD, G.parameters(), create_graph=True)\n",
    "    dLG_dD = torch.autograd.grad(lossG, D.parameters(), create_graph=True)\n",
    "    J_D_P_grad_G = torch.autograd.grad(dLD_dG, D.parameters(), grad_outputs = gradsG)\n",
    "    J_G_P_grad_D = torch.autograd.grad(dLG_dD, G.parameters(), grad_outputs = gradsD)\n",
    "\n",
    "    # Calculating the LookAhead Step\n",
    "    gradsD_LookAhead = detach_tuple(gradsD+J_D_P_grad_G * eta_lookahead)\n",
    "    gradsG_LookAhead = detach_tuple(gradsG+J_G_P_grad_D * eta_lookahead)\n",
    "\n",
    "    # Updating the Networks\n",
    "    for param, grad in zip(D.parameters(),gradsD_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    for param, grad in zip(G.parameters(),gradsG_LookAhead):\n",
    "        param.data -= grad*lr\n",
    "    if i%1000 == 0:\n",
    "        end = time()\n",
    "        print(\"iter: \", i)\n",
    "        print(\"lossD: \", lossD.item())\n",
    "        print(\"lossG: \", lossG.item())\n",
    "        print(\"time cost: \", end-start)\n",
    "        start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval\n",
    "z = torch.randn(args[\"batch_size\"], _NOISE_DIM, device=device)\n",
    "output = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2a5d6ff390>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0,0].cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
